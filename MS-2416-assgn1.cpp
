#include <iostream>
#include <vector>
#include <mpi.h>
using namespace std;

int main(int argc, char* argv[]) {
    int rank, size, N;
    vector<int> arr;   // Only root will store the full array
    int local_sum = 0, total_sum = 0;

    MPI_Init(&argc, &argv);  
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);  
    MPI_Comm_size(MPI_COMM_WORLD, &size);  

    if (rank == 0) {
        cout << "Enter number of elements (N): ";
        cin >> N;

        arr.resize(N);
        cout << "Enter " << N << " numbers:\n";
        for (int i = 0; i < N; i++) {
            cin >> arr[i];
        }
    }

    // Broadcast N to all processes
    MPI_Bcast(&N, 1, MPI_INT, 0, MPI_COMM_WORLD);

    // Calculate local sizes (handle case when N is not divisible by size)
    int local_n = N / size;
    int remainder = N % size;

    vector<int> sendcounts(size), displs(size);
    for (int i = 0; i < size; i++) {
        sendcounts[i] = local_n + (i < remainder ? 1 : 0);
        displs[i] = (i == 0) ? 0 : displs[i-1] + sendcounts[i-1];
    }

    // Allocate local buffer
    vector<int> sub_arr(sendcounts[rank]);

    // Scatter data unevenly (using MPI_Scatterv)
    MPI_Scatterv(arr.data(), sendcounts.data(), displs.data(), MPI_INT,
                 sub_arr.data(), sendcounts[rank], MPI_INT, 0, MPI_COMM_WORLD);

    // Each processor computes local sum
    for (int x : sub_arr) {
        local_sum += x;
    }

    // Reduce local sums to total sum at root
    MPI_Reduce(&local_sum, &total_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        cout << "Total sum = " << total_sum << endl;
    }

    MPI_Finalize();
    return 0;
}
